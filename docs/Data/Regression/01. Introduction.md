---
sidebar_position: 1
---

# Linear Regression Model
## Model
$Y_{i} = \beta_{0} + \beta_{1}x_{i}+\epsilon_{i}$
## Data
$(x_{1},y_{1})\cdots\cdots(x_{n},y_{n})$
- $Y_{i}$ : Response variable, **Random variable**
- $X_{i}$ : Independent/Expalanatory/Predictor/Regressor Variable, "Fixed Quantity"

## Parameters($\beta_{0}$, $\beta_{1}$, $\sigma^{2}$)
- $\beta_{0}$ : Unknown **Intercept**, ther predicted variable for $Y$ when $X=0$
- $\beta_{1}$ : **Slope**, represnets the change of $Y$ when $X$ increase by 1 Unit.
  - Posses an important information for the relation between $X$ and $Y$

- $\sigma^{2}$ : Error Variance plays and important role for statistical inference
  - Ex) Testing, Confidence Interval
# Key Assumptions
1) $X$ and $Y$ have a **linear** relationship
	- 선형성
	- 추후 2차 곡선과 같은 추정은 다른 형태의 수식을 사용
3) $\epsilon_{i}$ are independents
	- 독립성
	- $\Leftrightarrow$ $\epsilon_{i}$ do not depend on $i$
	- $\Leftrightarrow$ Constant Variance Assumption
5) $\epsilon_{i}$ are identically distributed
6) Additional Assumpation : $\epsilon_{i}$ are Normal
	- 정규 분포 가정 외에 그냥 평균과 분산만 가정하는 경우도 존재

# 최소제곱법(Least Square Estimation)
최소자승법이라고도 한다.

- $Q(\beta_{0},\beta_{1}) \overset{let}{=} \sum_{i = 1}^{n}(Y_{i}-\beta_{0}-\beta_{1}x_{i})^{2}$ 

## Normal Equation
### $\hat{\beta_{0}}$ Derivation
$$
\begin{align*}
\frac{\delta Q}{\delta\beta_{0}} &= -2 \sum_{i = 1}^{n}(y_{i}-\beta_{0}-\beta_{1}x_{i}) \overset{set}{=} 0 
\\
& \Leftrightarrow \sum_{i = 1}^{n}y_{i} - n \beta_{0}-\sum_{i = 1}^{n}\beta_{1}x_{i}) = 0 
\\
& \Leftrightarrow n \beta_{0} = \sum_{i = 1}^{n}y_{i} - \beta_{1} \sum_{i = 1}^{n}x_{i} 
\\
& \Leftrightarrow \beta_{0} = \frac{1}{n}\sum_{i = 1}^{n}y_{i} - \beta_{1} \frac{1}{n} \sum_{i = 1}^{n}x_{i} 
\\
& \therefore \hat{\beta_{0}} = \overline{Y} - \hat{\beta_{1}} \overline{X} 
\end{align*}
$$

### $\hat{\beta_{1}}$ Derivation
$$
\begin{align*}
\frac{\delta Q}{\delta\beta_{1}} &= -2 \sum_{i = 1}^{n}(y_{i}-\beta_{0}-\beta_{1}x_{i}) x_{i} \overset{set}{=} 0 &&
\\
& \Leftrightarrow \sum_{i = 1}^{n}x_{i}y_{i} - \beta_{0} \sum_{i = 1}^{n}x_{i}-\beta_{1} \sum_{i = 1}^{n}x_{i}^{2} = 0 &&
\\
& \because \beta_{0} = \overline{Y} - \beta_{1} \overline{X} &&
\\
& \Leftrightarrow \sum_{i = 1}^{n}x_{i}y_{i} - \overline{Y} \sum_{i = 1}^{n}x_{i} + \beta_{1} \overline{X} \sum_{i = 1}^{n}x_{i} -\beta_{1} \sum_{i = 1}^{n}x_{i}^{2} = 0 &&
\\
& \Leftrightarrow \sum_{i = 1}^{n}x_{i}y_{i} - n \overline{X} \overline{Y} + \beta_{1} \overline{X} \sum_{i = 1}^{n}x_{i} -\beta_{1} \sum_{i = 1}^{n}x_{i}^{2} = 0 &&
\\
& \Leftrightarrow \sum_{i = 1}^{n}x_{i}y_{i} - n \overline{X} \overline{Y} = \beta_{1}(\sum_{i = 1}^{n}x_{i}^{2} - n \overline{X}^{2} )  &&
\\
& \Leftrightarrow \beta_{1} = \frac{\sum_{i = 1}^{n}x_{i}y_{i} - n \overline{X} \overline{Y}}{\sum_{i = 1}^{n}x_{i}^{2} - n \overline{X}^{2}} &&
\\
& \therefore \hat{\beta_{1}} = \frac{\sum_{i = 1}^{n}x_{i}(y_{i} - \overline{Y})}{\sum_{i = 1}^{n}x_{i}( x_{i} - \overline{X})} = \frac{S_{xx}}{S_{xy}} &&
\end{align*}
$$