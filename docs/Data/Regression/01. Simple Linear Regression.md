---
sidebar_position: 1
---

# Simple Linear Regression

## Relationship between Two Numerical Variables

- When we have two numerical variables, say $X$ and $Y$, we are usually interested in whether or not they are **correlated**.
- This means that when $X$ increases, $Y$ also tends to increase (or vice versa) (**Positive association**).
- **Negative association** is when the value of $X$ decreases with the increase of $Y$.

## Data

$(x_{1},y_{1})\cdots\cdots(x_{n},y_{n})$
- $y_{i}$ : Response/Dependent variable. **Random variable**.
- $x_{i}$ : Independent/Expalanatory/Predictor/Regressor variable. **Fixed** quantity.

## Simple Linear Regression Model

$y_{i} = \beta_{0} + \beta_{1}x_{i}+\epsilon_{i}$ where $\epsilon_{i} \overset{iid}{\sim} (0,\sigma^{2})$
- $E(y_{i}) = E(\beta_{0} + \beta_{1}x_{i}+\epsilon_{i}) = \beta_{0} + \beta_{1}x_{i}$
- $Var(y_{i}) = Var(\beta_{0} + \beta_{1}x_{i}+\epsilon_{i}) = Var(\epsilon_{i}) = \sigma^{2}$
- $\therefore y_{i} \sim (\beta_{0} + \beta_{1} x_{i}, \sigma^{2})$

## Parameters($\beta_{0}$, $\beta_{1}$, $\sigma^{2}$)
- $\beta_{0}$ : Unknown **Intercept**, the expected value of $Y$ when $X=0$.
- $\beta_{1}$ : Unknown **Slope**, represents the change of $Y$ when $X$ increase by 1 Unit.
  - Posses an important information for the relationship between $X$ and $Y$.
- $\sigma^{2}$ : Error variance plays an important role for statistical inference
  - e.g. Hypothesis testing, confidence intervals, $\ldots$


## Key Assumptions
1) $X$ and $Y$ have a **linear** relationship
	- 선형성
	- 추후 2차 곡선과 같은 추정은 다른 형태의 수식을 사용
3) $\epsilon_{i}$ are independents
	- 독립성
	- $\Leftrightarrow$ $\epsilon_{i}$ do not depend on $i$
	- $\Leftrightarrow$ Constant Variance Assumption
5) $\epsilon_{i}$ are identically distributed
6) Additional Assumpation : $\epsilon_{i}$ are Normal
	- 정규 분포 가정 외에 그냥 평균과 분산만 가정하는 경우도 존재


## Least Square Estimation(LSE)
최소제곱법, 최소자승법이라고도 하며 약자로 LSE로 표기한다.
- $\hat{\beta_{0}} = \overline{Y} - \hat{\beta_{1}} \overline{X}$
- $\hat{\beta_{1}} = \frac{S_{xy}}{S_{xx}}$
  - $S_{xx} = \sum_{i = 1}^{n}x_{i}( x_{i} - \overline{X}) = \sum_{i = 1}^{n} ( x_{i} - \overline{X})^{2}$
  - $S_{yy} = \sum_{i = 1}^{n}y_{i}( y_{i} - \overline{Y}) = \sum_{i = 1}^{n} ( y_{i} - \overline{Y})^{2}$
  - $S_{xy} = \sum_{i = 1}^{n}x_{i}(y_{i} - \overline{Y}) = \sum_{i = 1}^{n}(x_{i} - \overline{X})(y_{i} - \overline{Y})$
  - $\hat{\beta_{1}} = \frac{\sum_{i = 1}^{n}x_{i}(y_{i} - \overline{Y})}{\sum_{i = 1}^{n}x_{i}( x_{i} - \overline{X})}$


## Predictd Value of $y_{i}$
- $\hat{y_{i}} = \hat{\beta_{0}} + \hat{\beta_{1}} x_{i}$


## Proof with Normal Equation
Minimize the following equation using the derivative.
$Q(\beta_{0},\beta_{1}) = SSE = \sum_{i = 1}^{n}(Y_{i}-\beta_{0}-\beta_{1}x_{i})^{2}$ 


### $\hat{\beta_{0}}$ Derivation
$$
\begin{align*}
\frac{\delta Q}{\delta \beta_{0}} &= -2 \sum_{i = 1}^{n}(y_{i} - \beta_{0} - \beta_{1} x_{i}) \overset{set}{=} 0 &&
\\
& \Leftrightarrow \sum_{i = 1}^{n}y_{i} - n \beta_{0} - \sum_{i = 1}^{n} \beta_{1} x_{i}) = 0 &&
\\
& \Leftrightarrow n \beta_{0} = \sum_{i = 1}^{n} y_{i} - \beta_{1} \sum_{i = 1}^{n} x_{i} &&
\\
& \Leftrightarrow \beta_{0} = \frac{1}{n} \sum_{i = 1}^{n} y_{i} - \beta_{1} \frac{1}{n} \sum_{i = 1}^{n} x_{i} && 
\\
& \therefore \hat{\beta_{0}} = \overline{Y} - \hat{\beta_{1}} \overline{X} &&
\end{align*}
$$


### $\hat{\beta_{1}}$ Derivation
$$
\begin{align*}
\frac{\delta Q}{\delta\beta_{1}} &= -2 \sum_{i = 1}^{n}(y_{i} - \beta_{0} - \beta_{1} x_{i}) x_{i} \overset{set}{=} 0 &&
\\
& \Leftrightarrow \sum_{i = 1}^{n} x_{i} y_{i} - \beta_{0} \sum_{i = 1}^{n} x_{i} - \beta_{1} \sum_{i = 1}^{n} x_{i}^{2} = 0 &&
\\
& \because \beta_{0} = \overline{Y} - \beta_{1} \overline{X} &&
\\
& \Leftrightarrow \sum_{i = 1}^{n} x_{i} y_{i} - \overline{Y} \sum_{i = 1}^{n} x_{i} + \beta_{1} \overline{X} \sum_{i = 1}^{n} x_{i} -\beta_{1} \sum_{i = 1}^{n} x_{i}^{2} = 0 &&
\\
& \Leftrightarrow \sum_{i = 1}^{n} x_{i} y_{i} - n \overline{X} \overline{Y} + \beta_{1} \overline{X} \sum_{i = 1}^{n} x_{i} -\beta_{1} \sum_{i = 1}^{n} x_{i}^{2} = 0 &&
\\
& \Leftrightarrow \sum_{i = 1}^{n} x_{i} y_{i} - n \overline{X} \overline{Y} = \beta_{1}(\sum_{i = 1}^{n} x_{i}^{2} - n \overline{X}^{2} )  &&
\\
& \Leftrightarrow \beta_{1} = \frac{\sum_{i = 1}^{n} x_{i} y_{i} - n \overline{X} \overline{Y}}{\sum_{i = 1}^{n} x_{i}^{2} - n \overline{X}^{2}} &&
\\
& \therefore \hat{\beta_{1}} = \frac{\sum_{i = 1}^{n} x_{i}(y_{i} - \overline{Y})}{\sum_{i = 1}^{n} x_{i}( x_{i} - \overline{X})} = \frac{S_{xy}}{S_{xx}} &&
\end{align*}
$$

## Properties of Estimators
### \beta_{0}

### \beta_{1}

