---
sidebar_position: 1
---

# Simple Linear Regression

## Relationship between Two Numerical Variables

When we have two numerical variables, say $X$ and $Y$, we are typically interested in whether they are **correlated** - that is, how $Y$ changes as $X$ changes.
- **Positive association**: As the value of $X$ increases, $Y$ tends to increase as well (or vice versa).
- **Negative association**: As the value of $X$ decreases, $Y$ tends to increase (or vice versa).

## Data

$(x_{1},Y_{1}), \cdots, (x_{n},Y_{n})$
- $Y_{i}$ : Response/Dependent variable. **Random variable**.
- $x_{i}$ : Independent/Expalanatory/Predictor/Regressor variable. **Fixed** quantity.

## Simple Linear Regression Model

$$
Y_{i} = \beta_{0} + \beta_{1}x_{i}+\epsilon_{i},
$$ 

where $\epsilon_{i} \overset{iid}{\sim} (0,\sigma^{2})$

- $E(Y_{i}) = E(\beta_{0} + \beta_{1}x_{i}+\epsilon_{i}) = \beta_{0} + \beta_{1}x_{i}$
- $Var(Y_{i}) = Var(\beta_{0} + \beta_{1}x_{i}+\epsilon_{i}) = Var(\epsilon_{i}) = \sigma^{2}$


## Parameters
- $\beta_{0}$ : Unknown **Intercept**, the expected value of $Y$ when $X=0$.
- $\beta_{1}$ : Unknown **Slope**, represents the change of $Y$ when $X$ increase by 1 Unit.
  - Posses an important information for the relationship between $X$ and $Y$.
- $\sigma^{2}$ : Error variance plays an important role for statistical inference
  - e.g. Hypothesis testing, confidence intervals, $\ldots$


## Key Assumptions

1. Linearity
	- $X$ and $Y$ have a **linear** relationship.
2. Independence
	- The error terms, $\epsilon_{i}$, are independent.
3. Constant variance
	- $Var(\epsilon_i) = \sigma^2 \Rightarrow \epsilon_{i}$ are identically distributed.
4. Normality (additional assumption)
	- $\epsilon_{i}$ are Normally distributed.


## Least Square Estimation(LSE)

Least Square Estimation is a method to estimate the unknown intercept $\beta_0$ and slope $\beta_1$ without using Normality assumption. The idea is to find the estimates that minimizes the sum of squared errors (SSE) $Q(\beta_{0},\beta_{1}) = \sum_{i = 1}^{n}(Y_{i}-\beta_{0}-\beta_{1}x_{i})^{2}$. Then,


- $\hat{\beta_{0}} = \overline{Y} - \hat{\beta_{1}} \overline{X}$
- $\hat{\beta_{1}} = \frac{S_{xy}}{S_{xx}}$
  - $S_{xx} = \sum_{i = 1}^{n}x_{i}( x_{i} - \overline{X}) = \sum_{i = 1}^{n} ( x_{i} - \overline{X})^{2}$
  - $S_{yy} = \sum_{i = 1}^{n}Y_{i}( Y_{i} - \overline{Y}) = \sum_{i = 1}^{n} ( Y_{i} - \overline{Y})^{2}$
  - $S_{xy} = \sum_{i = 1}^{n}x_{i}(Y_{i} - \overline{Y}) = \sum_{i = 1}^{n}(x_{i} - \overline{X})(Y_{i} - \overline{Y})$
  - $\hat{\beta_{1}} = \frac{\sum_{i = 1}^{n}x_{i}(Y_{i} - \overline{Y})}{\sum_{i = 1}^{n}x_{i}( x_{i} - \overline{X})}$

### Derivation of the estimators

$$
\begin{align*}
\frac{\delta Q}{\delta \beta_{0}} &= -2 \sum_{i = 1}^{n}(Y_{i} - \beta_{0} - \beta_{1} x_{i}) \overset{set}{=} 0 &&
\\
& \Leftrightarrow \sum_{i = 1}^{n}Y_{i} - n \beta_{0} - \sum_{i = 1}^{n} \beta_{1} x_{i}) = 0 &&
\\
& \Leftrightarrow n \beta_{0} = \sum_{i = 1}^{n} Y_{i} - \beta_{1} \sum_{i = 1}^{n} x_{i} &&
\\
& \Leftrightarrow \beta_{0} = \frac{1}{n} \sum_{i = 1}^{n} Y_{i} - \beta_{1} \frac{1}{n} \sum_{i = 1}^{n} x_{i} && 
\\
& \therefore \hat{\beta_{0}} = \overline{Y} - \hat{\beta_{1}} \overline{X} &&
\end{align*}
$$


### $\hat{\beta_{1}}$ Derivation
$$
\begin{align*}
\frac{\delta Q}{\delta\beta_{1}} &= -2 \sum_{i = 1}^{n}(Y_{i} - \beta_{0} - \beta_{1} x_{i}) x_{i} \overset{set}{=} 0 &&
\\
& \Leftrightarrow \sum_{i = 1}^{n} x_{i} Y_{i} - \beta_{0} \sum_{i = 1}^{n} x_{i} - \beta_{1} \sum_{i = 1}^{n} x_{i}^{2} = 0 &&
\\
& \because \beta_{0} = \overline{Y} - \beta_{1} \overline{X} &&
\\
& \Leftrightarrow \sum_{i = 1}^{n} x_{i} Y_{i} - \overline{Y} \sum_{i = 1}^{n} x_{i} + \beta_{1} \overline{X} \sum_{i = 1}^{n} x_{i} -\beta_{1} \sum_{i = 1}^{n} x_{i}^{2} = 0 &&
\\
& \Leftrightarrow \sum_{i = 1}^{n} x_{i} Y_{i} - n \overline{X} \overline{Y} + \beta_{1} \overline{X} \sum_{i = 1}^{n} x_{i} -\beta_{1} \sum_{i = 1}^{n} x_{i}^{2} = 0 &&
\\
& \Leftrightarrow \sum_{i = 1}^{n} x_{i} Y_{i} - n \overline{X} \overline{Y} = \beta_{1}(\sum_{i = 1}^{n} x_{i}^{2} - n \overline{X}^{2} )  &&
\\
& \Leftrightarrow \beta_{1} = \frac{\sum_{i = 1}^{n} x_{i} Y_{i} - n \overline{X} \overline{Y}}{\sum_{i = 1}^{n} x_{i}^{2} - n \overline{X}^{2}} &&
\\
& \therefore \hat{\beta_{1}} = \frac{\sum_{i = 1}^{n} x_{i}(Y_{i} - \overline{Y})}{\sum_{i = 1}^{n} x_{i}( x_{i} - \overline{X})} = \frac{S_{xy}}{S_{xx}} &&
\end{align*}
$$

## Predictd Values

With the given value of $x_i$, the predicted response can be found as 

$$\hat{Y_{i}} = \hat{\beta_{0}} + \hat{\beta_{1}} x_{i}$$





## Properties of Estimators

### Intercept

### Slope

