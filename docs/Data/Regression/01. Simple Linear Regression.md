---
sidebar_position: 1
---

# Simple Linear Regression

## Relationship between Two Numerical Variables

When we have two numerical variables, say $X$ and $Y$, we are typically interested in whether they are **correlated** - that is, how $Y$ changes as $X$ changes.
- **Positive association**: As the value of $X$ increases, $Y$ tends to increase as well (or vice versa).
- **Negative association**: As the value of $X$ decreases, $Y$ tends to increase (or vice versa).

## Data

$(x_{1},y_{1}), \cdots, (x_{n},y_{n})$
- $y_{i}$ : Response/Dependent variable. **Random variable**.
- $x_{i}$ : Independent/Expalanatory/Predictor/Regressor variable. **Fixed** quantity.

## Simple Linear Regression Model

$$y_{i} = \beta_{0} + \beta_{1}x_{i}+\epsilon_{i},$$ 
where $\epsilon_{i} \overset{iid}{\sim} (0,\sigma^{2})$

- $E(y_{i}) = E(\beta_{0} + \beta_{1}x_{i}+\epsilon_{i}) = \beta_{0} + \beta_{1}x_{i}$
- $Var(y_{i}) = Var(\beta_{0} + \beta_{1}x_{i}+\epsilon_{i}) = Var(\epsilon_{i}) = \sigma^{2}$


## Parameters
- $\beta_{0}$ : Unknown **Intercept**, the expected value of $Y$ when $X=0$.
- $\beta_{1}$ : Unknown **Slope**, represents the change of $Y$ when $X$ increase by 1 Unit.
  - Posses an important information for the relationship between $X$ and $Y$.
- $\sigma^{2}$ : Error variance plays an important role for statistical inference
  - e.g. Hypothesis testing, confidence intervals, $\ldots$


## Key Assumptions

1) Linearity
	- $X$ and $Y$ have a **linear** relationship.
2) Independence
	- The error terms, $\epsilon_{i}$, are independent.
3) Constant variance
	- $Var(\epsilon_i) = \sigma^2 \Rightarrow \epsilon_{i}$ are identically distributed.
4) Normality (\textcolor{red}{additional} assumption)
	- $\epsilon_{i}$ are Normally distributed.


## Least Square Estimation(LSE)

Least Square Estimation is a method to estimate the unknown intercept $\beta_0$ and slope $\beta_1$ without using Normality assumption. The idea is to find the estimates that minimizes the squared residuals $\sum_{i=1}^n (y_i - \hat y_i)^2$, where $\hat y_i = \hat\beta_0 + \hat\beta_1 x_i$. $\hat\beta_0$ and $\hat\beta_1$ are the estimates. 


- $\hat{\beta_{0}} = \overline{Y} - \hat{\beta_{1}} \overline{X}$
- $\hat{\beta_{1}} = \frac{S_{xy}}{S_{xx}}$
  - $S_{xx} = \sum_{i = 1}^{n}x_{i}( x_{i} - \overline{X}) = \sum_{i = 1}^{n} ( x_{i} - \overline{X})^{2}$
  - $S_{yy} = \sum_{i = 1}^{n}y_{i}( y_{i} - \overline{Y}) = \sum_{i = 1}^{n} ( y_{i} - \overline{Y})^{2}$
  - $S_{xy} = \sum_{i = 1}^{n}x_{i}(y_{i} - \overline{Y}) = \sum_{i = 1}^{n}(x_{i} - \overline{X})(y_{i} - \overline{Y})$
  - $\hat{\beta_{1}} = \frac{\sum_{i = 1}^{n}x_{i}(y_{i} - \overline{Y})}{\sum_{i = 1}^{n}x_{i}( x_{i} - \overline{X})}$


## Predictd Values
- $\hat{y_{i}} = \hat{\beta_{0}} + \hat{\beta_{1}} x_{i}$


## Proof with Normal Equation
Minimize the following equation using the derivative.
$Q(\beta_{0},\beta_{1}) = SSE = \sum_{i = 1}^{n}(Y_{i}-\beta_{0}-\beta_{1}x_{i})^{2}$ 


### $\hat{\beta_{0}}$ Derivation
$$
\begin{align*}
\frac{\delta Q}{\delta \beta_{0}} &= -2 \sum_{i = 1}^{n}(y_{i} - \beta_{0} - \beta_{1} x_{i}) \overset{set}{=} 0 &&
\\
& \Leftrightarrow \sum_{i = 1}^{n}y_{i} - n \beta_{0} - \sum_{i = 1}^{n} \beta_{1} x_{i}) = 0 &&
\\
& \Leftrightarrow n \beta_{0} = \sum_{i = 1}^{n} y_{i} - \beta_{1} \sum_{i = 1}^{n} x_{i} &&
\\
& \Leftrightarrow \beta_{0} = \frac{1}{n} \sum_{i = 1}^{n} y_{i} - \beta_{1} \frac{1}{n} \sum_{i = 1}^{n} x_{i} && 
\\
& \therefore \hat{\beta_{0}} = \overline{Y} - \hat{\beta_{1}} \overline{X} &&
\end{align*}
$$


### $\hat{\beta_{1}}$ Derivation
$$
\begin{align*}
\frac{\delta Q}{\delta\beta_{1}} &= -2 \sum_{i = 1}^{n}(y_{i} - \beta_{0} - \beta_{1} x_{i}) x_{i} \overset{set}{=} 0 &&
\\
& \Leftrightarrow \sum_{i = 1}^{n} x_{i} y_{i} - \beta_{0} \sum_{i = 1}^{n} x_{i} - \beta_{1} \sum_{i = 1}^{n} x_{i}^{2} = 0 &&
\\
& \because \beta_{0} = \overline{Y} - \beta_{1} \overline{X} &&
\\
& \Leftrightarrow \sum_{i = 1}^{n} x_{i} y_{i} - \overline{Y} \sum_{i = 1}^{n} x_{i} + \beta_{1} \overline{X} \sum_{i = 1}^{n} x_{i} -\beta_{1} \sum_{i = 1}^{n} x_{i}^{2} = 0 &&
\\
& \Leftrightarrow \sum_{i = 1}^{n} x_{i} y_{i} - n \overline{X} \overline{Y} + \beta_{1} \overline{X} \sum_{i = 1}^{n} x_{i} -\beta_{1} \sum_{i = 1}^{n} x_{i}^{2} = 0 &&
\\
& \Leftrightarrow \sum_{i = 1}^{n} x_{i} y_{i} - n \overline{X} \overline{Y} = \beta_{1}(\sum_{i = 1}^{n} x_{i}^{2} - n \overline{X}^{2} )  &&
\\
& \Leftrightarrow \beta_{1} = \frac{\sum_{i = 1}^{n} x_{i} y_{i} - n \overline{X} \overline{Y}}{\sum_{i = 1}^{n} x_{i}^{2} - n \overline{X}^{2}} &&
\\
& \therefore \hat{\beta_{1}} = \frac{\sum_{i = 1}^{n} x_{i}(y_{i} - \overline{Y})}{\sum_{i = 1}^{n} x_{i}( x_{i} - \overline{X})} = \frac{S_{xy}}{S_{xx}} &&
\end{align*}
$$

## Properties of Estimators
### \beta_{0}

### \beta_{1}

