---
sidebar_position: 2
---

# Simple Linear Regression without Intercept
## Model
$y_{i} = \beta_{1}x_{i}+\epsilon_{i}$ where $\epsilon_{i} \overset{iid}{\sim} (0,\sigma^{2})$
- $E(y_{i}) = E(\beta_{1}x_{i}+\epsilon_{i}) = \beta_{1}x_{i}$
- $Var(y_{i}) = Var(\beta_{1}x_{i}+\epsilon_{i}) = Var(\epsilon_{i}) = \sigma^{2}$
- $\therefore y_{i} \sim (\beta_{1} x_{i}, \sigma^{2})$

## Least Square Estimation(LSE)

## Predictd Value of $y_{i}$
- $\hat{y_{i}} = \hat{\beta_{0}} + \hat{\beta_{1}} x_{i}$

## Proof with Normal Equation
$Q(\beta_{1}) = SSE = \sum_{i = 1}^{n}(Y_{i}-\beta_{1}x_{i})^{2}$ 

### $\hat{\beta_{1}}$ Derivation
$$
\begin{align*}
\frac{\delta Q}{\delta\beta_{1}} &= -2 \sum_{i = 1}^{n}(y_{i} - \beta_{1} x_{i}) x_{i} \overset{set}{=} 0 &&
\\
& \Leftrightarrow \sum_{i = 1}^{n} x_{i} y_{i} - \beta_{1} \sum_{i = 1}^{n} x_{i}^{2} = 0 &&
\\
& \Leftrightarrow \sum_{i = 1}^{n} x_{i} y_{i} = \beta_{1} \sum_{i = 1}^{n} x_{i}^{2} &&
\\
& \therefore \hat{\beta_{1}} = \frac{\sum_{i = 1}^{n} x_{i} y_{i}}{\sum_{i = 1}^{n} x_{i}^2} &&
\end{align*}
$$

## Properties of Estimators